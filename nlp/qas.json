{
    "what are the different types of machine learning?": {
        "answer": "There are three types of machine learning:  Supervised Learning In supervised machine learning, a model makes predictions or decisions based on past or labeled data. Labeled data refers to sets of data that are given tags or labels, and thus made more meaningful.  Unsupervised Learning In unsupervised learning, we don't have labeled data. A model can identify patterns, anomalies, and relationships in the input data.  Reinforcement Learning Using reinforcement learning, the model can learn based on the rewards it received for its previous action.  Consider an environment where an agent is working. The agent is given a target to achieve. Every time the agent takes some action toward the target, it is given positive feedback. And, if the action taken is going away from the goal, the agent is given negative feedback.",
        "courses": "0",
        "toread": ["https://medium.com/datadriveninvestor/machine-learning-basics-part-1-361eb09418b5",
            "https://medium.com/@randylaosat/a-beginners-guide-to-machine-learning-dfadc19f6caf"]
    },
    "what is overfitting, and how can you avoid it?": {
        "answer": "Overfitting is a situation that occurs when a model learns the training set too well, taking up random fluctuations in the training data as concepts. These impact the model\u2019s ability to generalize and don\u2019t apply to new data.\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t When a model is given the training data, it shows 100 percent accuracy\u2014technically a slight loss. But, when we use the test data, there may be an error and low efficiency. This condition is known as overfitting.\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t There are multiple ways of avoiding overfitting, such as:\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t Regularization. It involves a cost term for the features involved with the objective function\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t Making a simple model. With lesser variables and parameters, the variance can be reduced\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t Cross-validation methods like k-folds can also be used\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t If some model parameters are likely to cause overfitting, techniques for regularization like LASSO can be used that penalize these parameters\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t"
    },
    "what is \u2018training set\u2019 and \u2018test set\u2019 in a machine learning model? how much data will you allocate for your training, validation, and test sets?": {
        "answer": "There is a three-step process followed to create a model:                  Train the model         Test the model         Deploy the model         Training Set        \"The training set is examples given to the model to analyze and learn 70% of the total data is typically taken as the training dataset This is labeled data used to train the model\"        Test Set  \"The test set is used to test the accuracy of the hypothesis generated by the model Remaining 30% is taken as testing dataset We test without labeled data and then verify results with labels\" Consider a case where you have labeled data for 1,000 records. One way to train the model is to expose all 1,000 records during the training process. Then you take a small set of the same data to test the model, which would give good results in this case.                  But, this is not an accurate way of testing. So, we set aside a portion of that data called the \u2018test set\u2019 before starting the training process. The remaining data is called the \u2018training set\u2019 that we use for training the model. The training set passes through the model multiple times until the accuracy is high, and errors are minimized.          Now, we pass the test data to check if the model can accurately predict the values and determine if training is effective. If you get errors, you either need to change your model or retrain it with more data.  Regarding the question of how to split the data into a training set and test set, there is no fixed rule, and the ratio can vary based on individual preferences."
    },
    "how do you handle missing or corrupted data in a dataset?": {
        "answer": "One of the easiest ways to handle missing or corrupted data is to drop those rows or columns or replace them entirely with some other value.  There are two useful methods in Pandas:  IsNull() and dropna() will help to find the columns/rows with missing data and drop them Fillna() will replace the wrong values with a placeholder value"
    },
    "how can you choose a classifier based on a training set data size?": {
        "answer": "When the training set is small, a model that has a right bias and low variance seem to work better because they are less likely to overfit.  For example, Naive Bayes works best when the training set is large. Models with low bias and high variance tend to perform better as they work fine with complex relationships."
    },
    "explain the confusion matrix with respect to machine learning algorithms.": {
        "answer": "A confusion matrix (or error matrix) is a specific table that is used to measure the performance of an algorithm. It is mostly used in supervised learning; in unsupervised learning, it\u2019s called the matching matrix.                                                                                                                                                                                                                                                                                                                                                                                                                  The confusion matrix has two parameters: Actual & Predicted                                                                                                                                                                                                         It also has identical sets of features in both of these dimensions.                                                                                                                                                                                                                                                                                                                                     For a model to be accurate, the values across the diagonals should be high. The total sum of all the values in the matrix equals the total observations in the test data set.                                                                                                                                                                                                                                                                                                                                                                                                                  Accuracy = sum of the values across the diagonal / total dataset "
    },
    "what is a false positive and false negative and how are they significant?": {
        "answer": "False positives are those cases which wrongly get classified as True but are False.  False negatives are those cases which wrongly get classified as False but are True.  In the term \u2018False Positive,\u2019 the word \u2018Positive\u2019 refers to the \u2018Yes\u2019 row of the predicted value in the confusion matrix. The complete term indicates that the system has predicted it as a positive, but the actual value is negative.  Similarly, in the term \u2018False Negative,\u2019 the word \u2018Negative\u2019 refers to the \u2018No\u2019 row of the predicted value in the confusion matrix. And the complete term indicates that the system has predicted it as negative, but the actual value is positive."
    },
    "what are the three stages of building a model in machine learning?": {
        "answer": "The three stages of building a machine learning model are:  Model Building Choose a suitable algorithm for the model and train it according to the requirement Model Testing Check the accuracy of the model through the test data Applying the Model Make the required changes after testing and use the final model for real-time projects Here, it\u2019s important to remember that once in a while, the model needs to be checked to make sure it\u2019s working correctly. It should be modified to make sure that it is up-to-date."
    },
    "what is deep learning?": {
        "answer": "Deep Learning involves taking large volumes of structured or unstructured data and using complex algorithms to train neural networks. It performs complex operations to extract hidden patterns and features (for instance, distinguishing the image of a cat from that of a dog)."
    },
    "what are the differences between machine learning and deep learning?": {
        "answer": "Machine Learning Enables machines to take decisions on their own, based on past data It needs only a small amount of data for training Works well on the low-end system, so you don't need large machines  Most features need to be identified in advance and manually coded The problem is divided into two parts and solved individually and then combined  Deep Learning Enables machines to take decisions with the help of artificial neural networks It needs a large amount of training data  Needs high-end machines because it requires a lot of computing power  The machine learns the features from the data it is provided The problem is solved in an end-to-end manner"
    },
    "what are the applications of supervised machine learning in modern businesses?": {
        "answer": "Applications of supervised machine learning include:  Email Spam Detection Here we train the model using historical data that consists of emails categorized as spam or not spam. This labeled information is fed as input to the model. Healthcare Diagnosis By providing images regarding a disease, a model can be trained to detect if a person is suffering from the disease or not. Sentiment Analysis This refers to the process of using algorithms to mine documents and determine whether they\u2019re positive, neutral, or negative in sentiment. Fraud Detection Training the model to identify suspicious patterns, we can detect instances of possible fraud."
    },
    "what is semi-supervised machine learning?": {
        "answer": "Supervised learning uses data that is completely labeled, whereas unsupervised learning uses no training data.  In the case of semi-supervised learning, the training data contains a small amount of labeled data and a large amount of unlabeled data."
    },
    "what are unsupervised machine learning techniques?": {
        "answer": "There are two techniques used in unsupervised learning: clustering and association.  Clustering Clustering problems involve data to be divided into subsets. These subsets, also called clusters, contain data that are similar to each other. Different clusters reveal different details about the objects, unlike classification or regression.  Association In an association problem, we identify patterns of associations between different variables or items.  For example, an ecommerce website can suggest other items for you to buy, based on the prior purchases that you have made, spending habits, items in your wishlist, other customers\u2019 purchase habits, and so on."
    },
    "what is the difference between supervised and unsupervised machine learning?": {
        "answer": "Supervised learning - This model learns from the labeled data and makes a future prediction as output Unsupervised learning - This model uses unlabeled input data and allows the algorithm to act on that information without guidance."
    },
    "what is the difference between inductive machine learning and deductive machine learning?": {
        "answer": "Inductive Learning It observes instances based on defined principles to draw a conclusion Example: Explaining to a child to keep away from the fire by showing a video where fire causes damage  Deductive Learning It concludes experiences Example: Allow the child to play with fire. If he or she gets burned, they will learn that it is dangerous and will refrain from making the same mistake again"
    },
    "compare k-means and knn algorithms.": {
        "answer": "\"K-Means is unsupervised K-Means is a clustering algorithm The points in each cluster are similar to each other, and each cluster is different from its neighboring clusters\"  KNN is supervised in nature KNN is a classification algorithm It classifies an unlabeled observation based on its K (can be any number) surrounding neighbors"
    },
    "what is \u2018naive\u2019 in the naive bayes classifier?": {
        "answer": "The classifier is called \u2018naive\u2019 because it makes assumptions that may or may not turn out to be correct.  The algorithm assumes that the presence of one feature of a class is not related to the presence of any other feature (absolute independence of features), given the class variable.  For instance, a fruit may be considered to be a cherry if it is red in color and round in shape, regardless of other features. This assumption may or may not be right (as an apple also matches the description)."
    },
    "explain how a system can play a game of chess using reinforcement learning.": {
        "answer": "Reinforcement learning has an environment and an agent. The agent performs some actions to achieve a specific goal. Every time the agent performs a task that is taking it towards the goal, it is rewarded. And, every time it takes a step which goes against that goal or in reverse direction, it is penalized.  Earlier, chess programs had to determine the best moves after much research on numerous factors. Building a machine designed to play such games would require many rules to be specified.  With reinforced learning, we don\u2019t have to deal with this problem as the learning agent learns by playing the game. It will make a move (decision), check if it\u2019s the right move (feedback), and keep the outcomes in memory for the next step it takes (learning). There is a reward for every correct decision the system takes and punishment for the wrong one."
    },
    "how will you know which machine learning algorithm to choose for your classification problem?": {
        "answer": "While there is no fixed rule to choose an algorithm for a classification problem, you can follow these guidelines:  If accuracy is a concern, test different algorithms and cross-validate them If the training dataset is small, use models that have low variance and high bias If the training dataset is large, use models that have high variance and little bias"
    },
    "how is amazon able to recommend other things to buy? how does the recommendation engine work?": {
        "answer": "Once a user buys something from Amazon, Amazon stores that purchase data for future reference and finds products that are most likely also to be bought, it is possible because of the Association algorithm, which can identify patterns in a given dataset."
    },
    "when will you use classification over regression?": {
        "answer": "Classification is used when your target is categorical, while regression is used when your target variable is continuous. Both classification and regression belong to the category of supervised machine learning algorithms.  Examples of classification problems include:  Predicting yes or no Estimating gender Breed of an animal Type of color Examples of regression problems include:  Estimating sales and price of a product Predicting the score of a team Predicting the amount of rainfall"
    },
    "how do you design an email spam filter?": {
        "answer": "Building a spam filter involves the following process:  The email spam filter will be fed with thousands of emails Each of these emails already has a label: \u2018spam\u2019 or \u2018not spam.\u2019 The supervised machine learning algorithm will then determine which type of emails are being marked as spam based on spam words like the lottery, free offer, no money, full refund, etc. The next time an email is about to hit your inbox, the spam filter will use statistical analysis and algorithms like Decision Trees and SVM to determine how likely the email is spam If the likelihood is high, it will label it as spam, and the email won\u2019t hit your inbox Based on the accuracy of each model, we will use the algorithm with the highest accuracy after testing all the models"
    },
    "what is a random forest?": {
        "answer": "A \u2018random forest\u2019 is a supervised machine learning algorithm that is generally used for classification problems. It operates by constructing multiple decision trees during the training phase. The random forest chooses the decision of the majority of the trees as the final decision."
    },
    "considering a long list of machine learning algorithms, given a data set, how do you decide which one to use?": {
        "answer": "There is no master algorithm for all situations. Choosing an algorithm depends on the following questions:  How much data do you have, and is it continuous or categorical? Is the problem related to classification, association, clustering, or regression? Predefined variables (labeled), unlabeled, or mix? What is the goal?"
    },
    "what is bias and variance in a machine learning model?": {
        "answer": "Bias Bias in a machine learning model occurs when the predicted values are further from the actual values. Low bias indicates a model where the prediction values are very close to the actual ones.  Underfitting: High bias can cause an algorithm to miss the relevant relations between features and target outputs.  Variance Variance refers to the amount the target model will change when trained with different training data. For a good model, the variance should be minimized.  Overfitting: High variance can cause an algorithm to model the random noise in the training data rather than the intended outputs"
    },
    "what is the trade-off between bias and variance?": {
        "answer": "The bias-variance decomposition essentially decomposes the learning error from any algorithm by adding the bias, variance, and a bit of irreducible error due to noise in the underlying dataset.  Necessarily, if you make the model more complex and add more variables, you\u2019ll lose bias but gain variance. To get the optimally-reduced amount of error, you\u2019ll have to trade off bias and variance. Neither high bias nor high variance is desired.  High bias and low variance algorithms train models that are consistent, but inaccurate on average.  High variance and low bias algorithms train models that are accurate but inconsistent."
    },
    "define precision and recall.": {
        "answer": "Precision Precision is the ratio of several events you can correctly recall to the total number of events you recall (mix of correct and wrong recalls).  Precision = (True Positive) / (True Positive + False Positive)  Recall A recall is the ratio of a number of events you can recall the number of total events.  Recall = (True Positive) / (True Positive + False Negative)"
    },
    "what is decision tree classification?": {
        "answer": "A decision tree builds classification (or regression) models as a tree structure, with datasets broken up into ever-smaller subsets while developing the decision tree, literally in a tree-like way with branches and nodes. Decision trees can handle both categorical and numerical data."
    },
    "what is pruning in decision trees, and how is it done?": {
        "answer": "Pruning is a technique in machine learning that reduces the size of decision trees. It reduces the complexity of the final classifier, and hence improves predictive accuracy by the reduction of overfitting.  Pruning can occur in:  Top-down fashion. It will traverse nodes and trim subtrees starting at the root Bottom-up fashion. It will begin at the leaf nodes There is a popular pruning algorithm called reduced error pruning, in which:  Starting at the leaves, each node is replaced with its most popular class If the prediction accuracy is not affected, the change is kept There is an advantage of simplicity and speed"
    },
    "briefly explain logistic regression.": {
        "answer": "Logistic regression is a classification algorithm used to predict a binary outcome for a given set of independent variables.  The output of logistic regression is either a 0 or 1 with a threshold value of generally 0.5. Any value above 0.5 is considered as 1, and any point below 0.5 is considered as 0."
    },
    "explain the k nearest neighbor algorithm.": {
        "answer": "K nearest neighbor algorithm is a classification algorithm that works in a way that a new data point is assigned to a neighboring group to which it is most similar.  In K nearest neighbors, K can be an integer greater than 1. So, for every new data point, we want to classify, we compute to which neighboring group it is closest.  Let us classify an object using the following example. Consider there are three clusters:  Football Basketball Tennis ball  Let the new data point to be classified is a black ball. We use KNN to classify it. Assume K = 5 (initially).  Next, we find the K (five) nearest data points, as shown.  Observe that all five selected points do not belong to the same cluster. There are three tennis balls and one each of basketball and football.  When multiple classes are involved, we prefer the majority. Here the majority is with the tennis ball, so the new data point is assigned to this cluster."
    },
    "what is a recommendation system?": {
        "answer": "Anyone who has used Spotify or shopped at Amazon will recognize a recommendation system: It\u2019s an information filtering system that predicts what a user might want to hear or see based on choice patterns provided by the user."
    },
    "what is kernel svm?": {
        "answer": "Kernel SVM is the abbreviated version of the kernel support vector machine. Kernel methods are a class of algorithms for pattern analysis, and the most common one is the kernel SVM."
    },
    "what are some methods of reducing dimensionality?": {
        "answer": "You can reduce dimensionality by combining features with feature engineering, removing collinear features, or using algorithmic dimensionality reduction."
    },
    "what is a neural network?": {
        "answer": "Neural Networks replicate the way humans learn, inspired by how the neurons in our brains fire, only much simpler.  The most common Neural Networks consist of three network layers:  An input layer A hidden layer (this is the most important layer where feature extraction takes place, and adjustments are made to train faster and function better) An output layer Each sheet contains neurons called \u201cnodes,\u201d performing various operations. Neural Networks are used in deep learning algorithms like CNN, RNN, GAN, etc."
    },
    "what is a multi-layer perceptron(mlp)?": {
        "answer": "As in Neural Networks, MLPs have an input layer, a hidden layer, and an output layer. It has the same structure as a single layer perceptron with one or more hidden layers. A single layer perceptron can classify only linear separable classes with binary output (0,1), but MLP can classify nonlinear classes.  Except for the input layer, each node in the other layers uses a nonlinear activation function. This means the input layers, the data coming in, and the activation function is based upon all nodes and weights being added together, producing the output. MLP uses a supervised learning method called \u201cbackpropagation.\u201d In backpropagation, the neural network calculates the error with the help of cost function. It propagates this error backward from where it came (adjusts the weights to train the model more accurately)."
    },
    "what is data normalization, and why do we need it?": {
        "answer": "The process of standardizing and reforming data is called \u201cData Normalization.\u201d It\u2019s a pre-processing step to eliminate data redundancy. Often, data comes in, and you get the same information in different formats. In these cases, you should rescale values to fit into a particular range, achieving better convergence."
    },
    "what is the boltzmann machine?": {
        "answer": "One of the most basic Deep Learning models is a Boltzmann Machine, resembling a simplified version of the Multi-Layer Perceptron. This model features a visible input layer and a hidden layer -- just a two-layer neural net that makes stochastic decisions as to whether a neuron should be on or off. Nodes are connected across layers, but no two nodes of the same layer are connected."
    },
    "what is the role of activation functions in a neural network?": {
        "answer": "At the most basic level, an activation function decides whether a neuron should be fired or not. It accepts the weighted sum of the inputs and bias as input to any activation function. Step function, Sigmoid, ReLU, Tanh, and Softmax are examples of activation functions."
    },
    "what is the cost function?": {
        "answer": "Also referred to as \u201closs\u201d or \u201cerror,\u201d cost function is a measure to evaluate how good your model\u2019s performance is. It\u2019s used to compute the error of the output layer during backpropagation. We push that error backward through the neural network and use that during the different training functions."
    },
    "what is gradient descent?": {
        "answer": "Gradient Descent is an optimal algorithm to minimize the cost function or to minimize an error. The aim is to find the local-global minima of a function. This determines the direction the model should take to reduce the error."
    },
    "what do you understand by backpropagation?": {
        "answer": "Backpropagation is a technique to improve the performance of the network. It backpropagates the error and updates the weights to reduce the error."
    },
    "what is the difference between a feedforward neural network and recurrent neural network?": {
        "answer": "A Feedforward Neural Network signals travel in one direction from input to output. There are no feedback loops; the network considers only the current input. It cannot memorize previous inputs (e.g., CNN).  A Recurrent Neural Network\u2019s signals travel in both directions, creating a looped network. It considers the current input with the previously received inputs for generating the output of a layer and can memorize past data due to its internal memory."
    },
    "what are the applications of a recurrent neural network (rnn)?": {
        "answer": "The RNN can be used for sentiment analysis, text mining, and image captioning. Recurrent Neural Networks can also address time series problems such as predicting the prices of stocks in a month or quarter."
    },
    "what are the softmax and relu functions?": {
        "answer": "Softmax is an activation function that generates the output between zero and one. It divides each output, such that the total sum of the outputs is equal to one. Softmax is often used for output layers.  ReLU (or Rectified Linear Unit) is the most widely used activation function. It gives an output of X if X is positive and zeroes otherwise. ReLU is often used for hidden layers."
    },
    "what are hyperparameters?": {
        "answer": "With neural networks, you\u2019re usually working with hyperparameters once the data is formatted correctly. A hyperparameter is a parameter whose value is set before the learning process begins. It determines how a network is trained and the structure of the network (such as the number of hidden units, the learning rate, epochs, etc.)."
    },
    "what will happen if the learning rate is set too low or too high?": {
        "answer": "When your learning rate is too low, training of the model will progress very slowly as we are making minimal updates to the weights. It will take many updates before reaching the minimum point.\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t If the learning rate is set too high, this causes undesirable divergent behavior to the loss function due to drastic updates in weights. It may fail to converge (model can give a good output) or even diverge (data is too chaotic for the network to train).\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t"
    },
    "what is dropout and batch normalization?": {
        "answer": "Dropout is a technique of dropping out hidden and visible units of a network randomly to prevent overfitting of data (typically dropping 20 percent of the nodes). It doubles the number of iterations needed to converge the network.\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t Batch normalization is the technique to improve the performance and stability of neural networks by normalizing the inputs in every layer so that they have mean output activation of zero and standard deviation of one.\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t"
    },
    "what is the difference between batch gradient descent and stochastic gradient descent?": {
        "answer": "Batch Gradient Descent \"The batch gradient computes the gradient using the entire dataset. It takes time to converge because the volume of data is huge, and weights update slowly.\"  Stochastic Gradient Descent \"The stochastic gradient computes the gradient using a single sample. It converges much faster than the batch gradient because it updates weight more frequently.\""
    },
    "what is overfitting and underfitting, and how to combat them?": {
        "answer": "Overfitting occurs when the model learns the details and noise in the training data to the degree that it adversely impacts the execution of the model on new information. It is more likely to occur with nonlinear models that have more flexibility when learning a target function. An example would be if a model is looking at cars and trucks, but only recognizes trucks that have a specific box shape. It might not be able to notice a flatbed truck because there's only a particular kind of truck it saw in training. The model performs well on training data, but not in the real world.  Underfitting alludes to a model that is neither well-trained on data nor can generalize to new information. This usually happens when there is less and incorrect data to train a model. Underfitting has both poor performance and accuracy.  To combat overfitting and underfitting, you can resample the data to estimate the model accuracy (k-fold cross-validation) and by having a validation dataset to evaluate the model."
    },
    "how are weights initialized in a network?": {
        "answer": "There are two methods here: we can either initialize the weights to zero or assign them randomly.  Initializing all weights to 0: This makes your model similar to a linear model. All the neurons and every layer perform the same operation, giving the same output and making the deep net useless.  Initializing all weights randomly: Here, the weights are assigned randomly by initializing them very close to 0. It gives better accuracy to the model since every neuron performs different computations. This is the most commonly used method."
    },
    "what are the different layers on cnn?": {
        "answer": "There are four layers in CNN:  Convolutional Layer - the layer that performs a convolutional operation, creating several smaller picture windows to go over the data. ReLU Layer - it brings non-linearity to the network and converts all the negative pixels to zero. The output is a rectified feature map. Pooling Layer - pooling is a down-sampling operation that reduces the dimensionality of the feature map. Fully Connected Layer - this layer recognizes and classifies the objects in the image."
    },
    "what is pooling on cnn, and how does it work?": {
        "answer": "Pooling is used to reduce the spatial dimensions of a CNN. It performs down-sampling operations to reduce the dimensionality and creates a pooled feature map by sliding a filter matrix over the input matrix."
    },
    "how does an lstm network work?": {
        "answer": "Long-Short-Term Memory (LSTM) is a special kind of recurrent neural network capable of learning long-term dependencies, remembering information for long periods as its default behavior. There are three steps in an LSTM network:  Step 1: The network decides what to forget and what to remember. Step 2: It selectively updates cell state values. Step 3: The network decides what part of the current state makes it to the output."
    },
    "what are vanishing and exploding gradients?": {
        "answer": "While training an RNN, your slope can become either too small or too large; this makes the training difficult. When the slope is too small, the problem is known as a \u201cVanishing Gradient.\u201d When the slope tends to grow exponentially instead of decaying, it\u2019s referred to as an \u201cExploding Gradient.\u201d Gradient problems lead to long training times, poor performance, and low accuracy."
    },
    "what is the difference between epoch, batch, and iteration in deep learning?": {
        "answer": "Epoch - Represents one iteration over the entire dataset (everything put into the training model). Batch - Refers to when we cannot pass the entire dataset into the neural network at once, so we divide the dataset into several batches. Iteration - if we have 10,000 images as data and a batch size of 200. then an epoch should run 50 iterations (10,000 divided by 50)."
    },
    "why is tensorflow the most preferred library in deep learning?": {
        "answer": "Tensorflow provides both C++ and Python APIs, making it easier to work on and has a faster compilation time compared to other Deep Learning libraries like Keras and Torch. Tensorflow supports both CPU and GPU computing devices."
    },
    "what do you mean by tensor in tensorflow?": {
        "answer": "A tensor is a mathematical object represented as arrays of higher dimensions. These arrays of data with different dimensions and ranks fed as input to the neural network are called \u201cTensors.\u201d"
    },
    "what are the programming elements in tensorflow?": {
        "answer": "Constants - Constants are parameters whose value does not change. To define a constant we use tf.constant() command. For example:  a = tf.constant(2.0,tf.float32)  b = tf.constant(3.0)  Print(a, b)  Variables - Variables allow us to add new trainable parameters to graph. To define a variable, we use the tf.Variable() command and initialize them before running the graph in a session. An example:  W = tf.Variable([.3].dtype=tf.float32)  b = tf.Variable([-.3].dtype=tf.float32)  Placeholders - these allow us to feed data to a tensorflow model from outside a model. It permits a value to be assigned later. To define a placeholder, we use the tf.placeholder() command. An example:  a = tf.placeholder (tf.float32)  b = a*2  with tf.Session() as sess:  result = sess.run(b,feed_dict={a:3.0})  print result  Sessions - a session is run to evaluate the nodes. This is called the \u201cTensorflow runtime.\u201d For example:  a = tf.constant(2.0)  b = tf.constant(4.0)  c = a+b  # Launch Session  Sess = tf.Session()  # Evaluate the tensor c  print(sess.run(c))"
    },
    "explain a computational graph.": {
        "answer": "Everything in a tensorflow is based on creating a computational graph. It has a network of nodes where each node operates, Nodes represent mathematical operations, and edges represent tensors. Since data flows in the form of a graph, it is also called a \u201cDataFlow Graph.\u201d"
    },
    "explain generative adversarial network.": {
        "answer": "Suppose there is a wine shop purchasing wine from dealers, which they resell later. But some dealers sell fake wine. In this case, the shop owner should be able to distinguish between fake and authentic wine.  The forger will try different techniques to sell fake wine and make sure specific techniques go past the shop owner\u2019s check. The shop owner would probably get some feedback from wine experts that some of the wine is not original. The owner would have to improve how he determines whether a wine is fake or authentic.  The forger\u2019s goal is to create wines that are indistinguishable from the authentic ones while the shop owner intends to tell if the wine is real or not accurately.  Let us understand this example with the help of an image shown above.  There is a noise vector coming into the forger who is generating fake wine.  Here the forger acts as a Generator.  The shop owner acts as a Discriminator.  The Discriminator gets two inputs; one is the fake wine, while the other is the real authentic wine. The shop owner has to figure out whether it is real or fake.  So, there are two primary components of Generative Adversarial Network (GAN) named:  Generator Discriminator The generator is a CNN that keeps keys producing images and is closer in appearance to the real images while the discriminator tries to determine the difference between real and fake images The ultimate aim is to make the discriminator learn to identify real and fake images."
    },
    "what is an auto-encoder?": {
        "answer": "This Neural Network has three layers in which the input neurons are equal to the output neurons. The network's target outside is the same as the input. It uses dimensionality reduction to restructure the input. It works by compressing the image input to a latent space representation then reconstructing the output from this representation."
    },
    "what is bagging and boosting?": {
        "answer": "Bagging and Boosting are ensemble techniques to train multiple models using the same learning algorithm and then taking a call.  With Bagging, we take a dataset and split it into training data and test data. Then we randomly select data to place into the bags and train the model separately.  With Boosting, the emphasis is on selecting data points which give wrong output to improve the accuracy."
    }
}